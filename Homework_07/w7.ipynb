{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# 1 check out this link to see how that’s done in a really easy way with the yield statement! \n",
    "# 2 You can just use a for loop iterating over num samples.\n",
    "# 3 You may want to make use of np.random.normal(size=).\n",
    "# 4 You can use np.sum(axis=) along the last axis and compare it to 0. Remember your output should be at best of type ’integer’ not ’boolean’\n",
    "# 5 Use np.expand dims(,-1) before you yield. The output shapes should be (seq len,1) and (1) for input and target respectiveley.\n",
    "# 6 For example, we chose 80.000.\n",
    "# 7 Shuffling, Batching.... As always.\n",
    "# 8 You can use a Dense layer for each gate with a hidden size specified by units and a sigmoid activation.\n",
    "# 9 To do so, check out the argument bias initializer= of the keras Dense layer impleme- nation and the tf.keras.initializers objects.\n",
    "# 10 For the concatenation of the input and the hidden state to pass on to the forget gate, you can make use of tf.concat( (), axis=).\n",
    "# 11 The appropiate size would be bacth size, cell units and you may want to use tf.zeros()\n",
    "# 12 Consider that you have a binary classification task. How many units should your readout layer haven and what activation function should you use?\n",
    "# 13 Do not forget to initialize the LSTM states with zeros each time you call your model and pass those on to your LSTM\n",
    "# 14 You may want to use slicing, -1 is always the last irrespective of the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b33927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 20:12:32.609240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-12 20:12:32.609272: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1.1\n",
    "\n",
    "\n",
    "def integration_task(seq_len, num_samples):\n",
    "    # for num_samples times yield random signal of size seq_len\n",
    "    # and a target (sum of noise signal is > or < than 1)\n",
    "    target = 0\n",
    "    \n",
    "    for i in num_samples:\n",
    "        target = target + i\n",
    "        ran = np.random.normal(size=seq_len)\n",
    "\n",
    "        \n",
    "    if np.sum(axis=target) > 0:\n",
    "        tar_check = 1\n",
    "        ran = np.expand_dims(ran,-1)\n",
    "        yield ran, tar_check \n",
    "    else:\n",
    "        yield 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def my_integration_task():\n",
    "    \n",
    "    num_samples = 80000\n",
    "    seq_len = 25\n",
    "    \n",
    "    yield integration_task(seq_len, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afda4b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<FlatMapDataset shapes: (20, 10), types: tf.float32>, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18662/2527288272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_integration_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iannwtf/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2417\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iannwtf/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \"\"\"\n\u001b[1;32m    369\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iannwtf/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iannwtf/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iannwtf/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             )\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(<FlatMapDataset shapes: (20, 10), types: tf.float32>, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "# ToDo: split data into training and test data\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(my_integration_task, output_signature = tf.TensorSpec([20,10],dtype=tf.float32))\n",
    "\n",
    "#train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=25)\n",
    "\n",
    "\n",
    "def prepare_data(ds):\n",
    "    return ds.shuffle(1000).batch(32).prefetch(20)\n",
    "                                         \n",
    "dataset = prepare_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f7a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - The network\n",
    "\n",
    "class LSTM_Cell(): # hidden_state, cell_state\n",
    "    \n",
    "    def __init__(self, units, unit_forget_bias = False):\n",
    "        \n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(units, kernel_regularizer=kernel_regularizer, use_bias=False, activation = tf.nn.sigmoid)\n",
    "        self.dense2 = tf.keras.layers.Dense(units, kernel_regularizer=kernel_regularizer, use_bias=False, activation = tf.nn.tanh)\n",
    "         \n",
    "        self.bias = tf.Variable(tf.zeros(units), name=\"LSTM_Cell_biases\")\n",
    "        \n",
    "        # for forget gate:\n",
    "        if unit_forget_bias == True:\n",
    "            bias +1\n",
    "            \n",
    "        weights = list()\n",
    "        \n",
    "        \n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        \n",
    "        # point wise multiplication of layers:\n",
    "        # to_add_cell = np.dot(self.dense1, self.dense2)\n",
    "        \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def call(self, x, states): # tf.concat( (), axis=)\n",
    "        \n",
    "        prev_output = states[0]\n",
    "                \n",
    "        to_output = np.dot(pre_output * self.dense1) +  self.bias\n",
    "            \n",
    "        # out = tf.nn.tanh(to_output)\n",
    "        \n",
    "        return to_output\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class LSTM_Layer():\n",
    "    \n",
    "    def __init__(self, cell):\n",
    "        \n",
    "        cell = LSTM_Cell(32, False)\n",
    "    \n",
    "    \n",
    "    def call(self,x,states):\n",
    "        # input is shape [batch_size, seq_len, input_size]\n",
    "        # output is shape [batch_size, seq_len, output_size]\n",
    "        \n",
    "        for i in states:\n",
    "            \n",
    "        \n",
    "        output_size = states[0]\n",
    "        \n",
    "        to_out = [x[0], x[1], output_size]\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def zero_states(self, batch_size):\n",
    "        \n",
    "        size = batch_size[0]\n",
    "        zero_state = np.zeros((size,),dtype='i,i')\n",
    "        \n",
    "        return zero_state\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class LSTM_Model():\n",
    "    \n",
    "    def __init__():\n",
    "        \n",
    "        # put into Layer?\n",
    "        read_in_layer = tf.keras.layers.Dense(128, activation = \"sigmoid\")\n",
    "        output_layer = tf.keras.layers.Dense(10, activation = \"tanh\")\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        out = self.read_in_layer(x)\n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
