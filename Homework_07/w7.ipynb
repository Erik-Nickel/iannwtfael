{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q9fd4CTJZKDG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Activation, Concatenate, LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xK3WIxszbeha"
      },
      "outputs": [],
      "source": [
        "def integration_task(seq_len,num_samples):\n",
        "  for i in range(num_samples):\n",
        "    seq = np.random.normal(size=seq_len)\n",
        "    if  seq.sum(axis=-1) <1:\n",
        "      target = 1\n",
        "    else:\n",
        "      target = 0\n",
        "    yield(np.expand_dims(seq,-1),target) \n",
        "\n",
        "seqlen =25\n",
        "samples = 100000\n",
        "\n",
        "def my_integration_task():\n",
        "  for i in integration_task(seqlen,samples):\n",
        "    yield(i)\n",
        "\n",
        "ds_1 = tf.data.Dataset.from_generator(my_integration_task, output_signature = (tf.TensorSpec(shape=(seqlen,1),dtype=tf.dtypes.float32),tf.TensorSpec(shape=(),dtype=tf.dtypes.float32)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHZzrq27ZSvO",
        "outputId": "34cb4bf0-0a94-4ee7-8490-ce4741d89fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 starting with accuracy 0.5887\n",
            "Epoch: 1 starting with accuracy 0.572778125\n"
          ]
        }
      ],
      "source": [
        "def prepare_data(ds):\n",
        "    return ds.shuffle(1024).batch(32).prefetch(32)  # TODO finish!!!!\n",
        "\n",
        "\n",
        "class LstmLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, LSTM_Cell, return_sequences=False):\n",
        "        super(LstmLayer, self).__init__()\n",
        "        self.return_sequences = return_sequences\n",
        "        self.cell = LSTM_Cell\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, data, training=False):\n",
        "        length = data.shape[1]\n",
        "        h_state = tf.zeros((data.shape[0], self.cell.units), tf.float32)\n",
        "        c_state = tf.zeros((data.shape[0], self.cell.units), tf.float32)\n",
        "        hidden_states = tf.TensorArray(dtype=tf.float32, size=length)\n",
        "        for t in tf.range(length):\n",
        "            input_t = data[:, t, :]\n",
        "            h_state, c_state = self.cell(input_t, h_state, c_state, training)\n",
        "            if self.return_sequences:\n",
        "                hidden_states.write(t, h_state)\n",
        "        if self.return_sequences:\n",
        "            # transpose the sequence of hidden_states from TensorArray accordingly\n",
        "            # (batch and time dimensions are otherwise switched after .stack())\n",
        "            outputs = tf.transpose(hidden_states.stack(), [1, 0, 2])\n",
        "        else:\n",
        "            outputs = h_state\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class LstmCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, kernel_regularizer=None):\n",
        "        super(LstmCell, self).__init__()\n",
        "        self.units = units\n",
        "        self.forget_gate = Dense(units, activation='sigmoid')\n",
        "        self.input_gate = Dense(units, activation='sigmoid')\n",
        "        self.cell_state_candidates = Dense(units, activation='tanh')\n",
        "        self.output_gate = Dense(units, activation='sigmoid')\n",
        "        self.tanh_layer = Activation(activation='tanh')\n",
        "        self.concat_layer = Concatenate(axis=-1)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x_t, h_t, c_t, training=False):\n",
        "        x_t_h_t = self.concat_layer([h_t, x_t])\n",
        "        f = self.forget_gate(x_t_h_t)\n",
        "        i = self.input_gate(x_t_h_t)\n",
        "        c_h = self.cell_state_candidates(x_t_h_t)\n",
        "        o = self.output_gate(x_t_h_t)\n",
        "        c = (f * c_t) + (i * c_h)\n",
        "        h = o * self.tanh_layer(c)\n",
        "        return h, c\n",
        "\n",
        "\n",
        "class LstmModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LstmModel, self).__init__()\n",
        "        units = 128 \n",
        "        self.LSTM1 = LstmLayer(LstmCell(units), return_sequences=False)\n",
        "        self.dense = Dense(128, activation=tf.nn.sigmoid)\n",
        "        self.out = Dense(1, activation=tf.nn.sigmoid)\n",
        "  \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "     \n",
        "        x = self.LSTM1(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(input)\n",
        "        loss = loss_function(target, prediction)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "    test_accuracy_aggregator = []\n",
        "    test_loss_aggregator = []\n",
        "\n",
        "    for (input, target) in test_data:\n",
        "        prediction = model(input)\n",
        "        sample_test_loss = loss_function(target, prediction)\n",
        "        sample_test_accuracy = np.round(target, 0) == np.round(prediction, 0)\n",
        "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "train_size = np.int64(samples*.8)\n",
        "test_size = np.int64(samples*.2)\n",
        "train_ds = ds_1.take(train_size)\n",
        "test_ds = ds_1.skip(train_size).take(test_size)\n",
        "  \n",
        "train_dataset = train_ds.apply(prepare_data)\n",
        "test_dataset = test_ds.apply(prepare_data)\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.01  \n",
        "\n",
        "model = LstmModel()\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "test_loss, test_accuracy = test(model, test_dataset, loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "train_loss, _ = test(model, train_dataset, loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "    epoch_loss_agg = []\n",
        "    for input, target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "    test_loss, test_accuracy = test(model, test_dataset, loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"training\")\n",
        "plt.plot(test_losses, label=\"test\")\n",
        "plt.plot(test_accuracies, label=\"test accuracy\")\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "w7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
